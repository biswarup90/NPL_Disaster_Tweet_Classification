{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce71676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "pd.set_option('display.max_columns', None)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn import feature_extraction, linear_model,  model_selection, preprocessing\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re # regular expression\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8aedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0cb61",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7ef8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ambiguous_labels(df):\n",
    "    df['target_relabeled'] = df['target'].copy()\n",
    "\n",
    "    df.loc[df[\n",
    "               'text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target_relabeled'] = 0\n",
    "    df.loc[df['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
    "    df.loc[df[\n",
    "               'text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n",
    "    df.loc[df[\n",
    "               'text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n",
    "    df.loc[df[\n",
    "               'text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n",
    "    df.loc[df[\n",
    "               'text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
    "    df.loc[\n",
    "        df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
    "    df.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n",
    "    df.loc[df[\n",
    "               'text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n",
    "    df.loc[df[\n",
    "               'text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0\n",
    "\n",
    "    df['target'] = df['target_relabeled'].copy()\n",
    "    df = df.drop(columns='target_relabeled')\n",
    "    return df\n",
    "def remove_stop_words(text):\n",
    "    filtered_stop_words = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct: # we use token attribute .is_stop\n",
    "            filtered_stop_words.append(token.text)\n",
    "    return \" \".join(filtered_stop_words)\n",
    "\n",
    "def lemmatized_string(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_string = []\n",
    "    for token in doc:\n",
    "        lemmatized_string.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_string)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    mean = words_vecs.mean(axis=0)\n",
    "    return mean\n",
    "def extract_label(text):\n",
    "    text = text.get('label')\n",
    "    if(text == 'LABEL_1'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1fccc",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7696c506",
   "metadata": {},
   "source": [
    "We are doing the following pre-processing:\n",
    "1. Some of the texts had been labelled twice & ambiguously. Once having disaster = true & again as disaster = false. Remove such ambiguous labels.\n",
    "2. Drop columns keyword, location & id because I did not use them\n",
    "3. Remove URL, HTML, punctuation & emoji\n",
    "4. Remove stop words - obtained from spacy\n",
    "5. Lemmatize the String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67c83dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    7613 non-null   object\n",
      " 1   target  7613 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 119.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train = remove_ambiguous_labels(df_train)\n",
    "df_train.drop(['id', 'keyword', 'location'], axis=1, inplace=True)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c8e4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009b5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text']= df_train['text'].apply(remove_URL)\n",
    "df_train['text']= df_train['text'].apply(remove_html)\n",
    "df_train['text']= df_train['text'].apply(remove_emoji)\n",
    "df_train['text']= df_train['text'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe512fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_without_stop_words'] = df_train['text'].apply(remove_stop_words)\n",
    "df_test['text_without_stop_words'] = df_test['text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbedd15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train['text_with_lemmatization'] = df_train['text_without_stop_words'].apply(lemmatized_string)\n",
    "df_test['text_with_lemmatization'] = df_test['text_without_stop_words'].apply(lemmatized_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b078417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_without_stop_words</th>\n",
       "      <th>text_with_lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>Deeds Reason earthquake ALLAH Forgive</td>\n",
       "      <td>deed Reason earthquake ALLAH Forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>forest fire near La Ronge Sask Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo Ruby Alaska smoke wildfires pou...</td>\n",
       "      <td>got send photo Ruby Alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this earthquake Ma...       1   \n",
       "1              Forest fire near La Ronge Sask Canada       1   \n",
       "2  All residents asked to shelter in place are be...       1   \n",
       "3  13000 people receive wildfires evacuation orde...       1   \n",
       "4  Just got sent this photo from Ruby Alaska as s...       1   \n",
       "\n",
       "                             text_without_stop_words  \\\n",
       "0              Deeds Reason earthquake ALLAH Forgive   \n",
       "1              Forest fire near La Ronge Sask Canada   \n",
       "2  residents asked shelter place notified officer...   \n",
       "3  13000 people receive wildfires evacuation orde...   \n",
       "4  got sent photo Ruby Alaska smoke wildfires pou...   \n",
       "\n",
       "                             text_with_lemmatization  \n",
       "0               deed Reason earthquake ALLAH Forgive  \n",
       "1              forest fire near La Ronge Sask Canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  13000 people receive wildfire evacuation order...  \n",
       "4  got send photo Ruby Alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680ade8",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a4073",
   "metadata": {},
   "source": [
    "Experimented with two ways to vectorize - \n",
    "1. scikit-learn's CountVectorizer\n",
    "2. scikit-learn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7de69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#count_vectorizer = feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "#train_vectors = count_vectorizer.fit_transform(df_train[\"text_with_lemmatization\"])\n",
    "#test_vectors = count_vectorizer.transform(df_test[\"text_with_lemmatization\"])\n",
    "tfIdf_vectorizer =  TfidfVectorizer()\n",
    "train_vectors = tfIdf_vectorizer.fit_transform(df_train[\"text_with_lemmatization\"])\n",
    "test_vectors = tfIdf_vectorizer.transform(df_test[\"text_with_lemmatization\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93632872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.todense().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11674f",
   "metadata": {},
   "source": [
    "### Train & Validate after Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7432fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63113006, 0.61248761, 0.67786561])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = linear_model.RidgeClassifier()\n",
    "# clf =  RandomForestClassifier()\n",
    "clf = MultinomialNB() #Best Performing\n",
    "# clf = BaggingClassifier()\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, df_train[\"target\"], cv=3, scoring=\"f1\")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002bf8f",
   "metadata": {},
   "source": [
    "### Tokenization - Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, df_train[\"target\"])\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_vectors)\n",
    "sample_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdc409",
   "metadata": {},
   "source": [
    "## Embedding - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f1ae660",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence.split() for sentence in df_train['text_with_lemmatization']]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c35500a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "75a4286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model.wv.most_similar('Massacre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1b841ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([vectorize(sentence) for sentence in df_train['text_with_lemmatization']])\n",
    "x_test = np.array([vectorize(sentence) for sentence in df_test['text_with_lemmatization']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a74e3",
   "metadata": {},
   "source": [
    "### Train & Evaluate after Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9f57d3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04468275, 0.05938865, 0.17171717])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = MultinomialNB() #Best Performing\n",
    "#clf = BaggingClassifier()\n",
    "clf = linear_model.RidgeClassifier()\n",
    "scores = model_selection.cross_val_score(clf, x_train, df_train[\"target\"], cv=3, scoring=\"f1\")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2803b9",
   "metadata": {},
   "source": [
    "### Word2Vec - Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c88ad8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_vectors, df_train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dfb945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_vectors)\n",
    "sample_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd475d2",
   "metadata": {},
   "source": [
    "## Hugging Face with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e76a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at hkayesh/twitter-disaster-nlp were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at hkayesh/twitter-disaster-nlp and are newly initialized: ['dropout_139']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "191/191 [==============================] - 198s 1s/step - loss: 0.4075 - accuracy: 0.8300 - val_loss: 0.3933 - val_accuracy: 0.8319\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 196s 1s/step - loss: 0.2694 - accuracy: 0.8974 - val_loss: 0.4669 - val_accuracy: 0.8247\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 195s 1s/step - loss: 0.1639 - accuracy: 0.9406 - val_loss: 0.6249 - val_accuracy: 0.8096\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 196s 1s/step - loss: 0.1046 - accuracy: 0.9627 - val_loss: 0.9086 - val_accuracy: 0.7511\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 196s 1s/step - loss: 0.0948 - accuracy: 0.9650 - val_loss: 0.7535 - val_accuracy: 0.7945\n",
      "48/48 [==============================] - 15s 299ms/step\n",
      "Fold 1 - Validation Accuracy: 0.7945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at hkayesh/twitter-disaster-nlp were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at hkayesh/twitter-disaster-nlp and are newly initialized: ['dropout_159']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "191/191 [==============================] - 195s 1s/step - loss: 0.4160 - accuracy: 0.8296 - val_loss: 0.4403 - val_accuracy: 0.8273\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 193s 1s/step - loss: 0.2741 - accuracy: 0.8910 - val_loss: 0.4250 - val_accuracy: 0.8496\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 191s 1s/step - loss: 0.1693 - accuracy: 0.9355 - val_loss: 0.5142 - val_accuracy: 0.8234\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 192s 1s/step - loss: 0.1148 - accuracy: 0.9604 - val_loss: 0.5419 - val_accuracy: 0.8286\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 192s 1s/step - loss: 0.0948 - accuracy: 0.9655 - val_loss: 0.6798 - val_accuracy: 0.7997\n",
      "48/48 [==============================] - 12s 235ms/step\n",
      "Fold 2 - Validation Accuracy: 0.7997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at hkayesh/twitter-disaster-nlp were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at hkayesh/twitter-disaster-nlp and are newly initialized: ['dropout_179']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "191/191 [==============================] - 196s 1s/step - loss: 0.4111 - accuracy: 0.8276 - val_loss: 0.3963 - val_accuracy: 0.8293\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 193s 1s/step - loss: 0.2724 - accuracy: 0.8951 - val_loss: 0.4240 - val_accuracy: 0.8319\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 194s 1s/step - loss: 0.1779 - accuracy: 0.9351 - val_loss: 0.5294 - val_accuracy: 0.8247\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 194s 1s/step - loss: 0.1231 - accuracy: 0.9550 - val_loss: 0.6550 - val_accuracy: 0.8234\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 194s 1s/step - loss: 0.0980 - accuracy: 0.9670 - val_loss: 0.6703 - val_accuracy: 0.8083\n",
      "48/48 [==============================] - 15s 296ms/step\n",
      "Fold 3 - Validation Accuracy: 0.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at hkayesh/twitter-disaster-nlp were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at hkayesh/twitter-disaster-nlp and are newly initialized: ['dropout_199']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "191/191 [==============================] - 195s 1s/step - loss: 0.4058 - accuracy: 0.8296 - val_loss: 0.3987 - val_accuracy: 0.8357\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 191s 1s/step - loss: 0.2905 - accuracy: 0.8852 - val_loss: 0.4099 - val_accuracy: 0.8292\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 192s 1s/step - loss: 0.1740 - accuracy: 0.9374 - val_loss: 0.4834 - val_accuracy: 0.8206\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 192s 1s/step - loss: 0.1164 - accuracy: 0.9583 - val_loss: 0.6083 - val_accuracy: 0.8147\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 193s 1s/step - loss: 0.0830 - accuracy: 0.9718 - val_loss: 0.4629 - val_accuracy: 0.8233\n",
      "48/48 [==============================] - 13s 265ms/step\n",
      "Fold 4 - Validation Accuracy: 0.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at hkayesh/twitter-disaster-nlp were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at hkayesh/twitter-disaster-nlp and are newly initialized: ['dropout_219']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "191/191 [==============================] - 197s 1s/step - loss: 0.4206 - accuracy: 0.8206 - val_loss: 0.4203 - val_accuracy: 0.8180\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 191s 1s/step - loss: 0.2919 - accuracy: 0.8852 - val_loss: 0.4196 - val_accuracy: 0.8390\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 195s 1s/step - loss: 0.1845 - accuracy: 0.9306 - val_loss: 0.5141 - val_accuracy: 0.8141\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 195s 1s/step - loss: 0.1227 - accuracy: 0.9553 - val_loss: 0.5709 - val_accuracy: 0.8265\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 193s 1s/step - loss: 0.0909 - accuracy: 0.9668 - val_loss: 0.5379 - val_accuracy: 0.7884\n",
      "48/48 [==============================] - 12s 245ms/step\n",
      "Fold 5 - Validation Accuracy: 0.7884\n",
      "Mean Accuracy across all folds: 0.8028\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# Load your DataFrame with two columns: 'text' and 'label'\n",
    "data = df_train  # Replace with your dataset file\n",
    "\n",
    "# Define hyperparameters for the model and training\n",
    "HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.RealInterval(1e-5, 1e-3))\n",
    "HP_NUM_EPOCHS = hp.HParam(\"num_epochs\", hp.IntInterval(3, 10))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer(\"logs/hparam_tuning\").as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LEARNING_RATE, HP_NUM_EPOCHS],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )\n",
    "\n",
    "# Define a function to build and compile the model\n",
    "def build_model(hparams):\n",
    "    model_name = \"hkayesh/twitter-disaster-nlp\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE]),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define cross-validation using Stratified K-Fold\n",
    "num_splits = 5  # Adjust the number of splits as needed\n",
    "skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "hparams = {\n",
    "    HP_LEARNING_RATE: 1e-4,  # Adjust your learning rate here\n",
    "    HP_NUM_EPOCHS: 5,        # Adjust the number of epochs here\n",
    "}\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(data['text_with_lemmatization'], data['target'])):\n",
    "    train_data, val_data = data.iloc[train_index], data.iloc[val_index]\n",
    "\n",
    "    model = build_model(hparams)\n",
    "    \n",
    "    # Tokenize the text data\n",
    "    train_inputs = tokenizer(list(train_data['text_with_lemmatization']), return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "    val_inputs = tokenizer(list(val_data['text_with_lemmatization']), return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    # Prepare the labels\n",
    "    train_labels = np.array(train_data['target'])\n",
    "    val_labels = np.array(val_data['target'])\n",
    "\n",
    "    # Define TensorBoard callbacks for visualization\n",
    "    log_dir = \"logs/fit/\" + f\"fold_{fold}\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_inputs.data,\n",
    "        train_labels,\n",
    "        validation_data=(val_inputs.data, val_labels),\n",
    "        epochs=hparams[HP_NUM_EPOCHS],\n",
    "        batch_size=32,\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_predictions = model.predict(val_inputs.data)\n",
    "    val_predicted_classes = np.argmax(val_predictions.logits, axis=1)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predicted_classes)\n",
    "    accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Calculate and report mean accuracy across all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy across all folds: {mean_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a4591",
   "metadata": {},
   "source": [
    "### Hugging Face with Tokenizer - Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea58366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 39s 379ms/step\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "unseen_inputs = tokenizer(list(df_test['text_with_lemmatization']), return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Make predictions on the unseen data\n",
    "unseen_predictions = model.predict(unseen_inputs.data)\n",
    "unseen_predicted_classes = np.argmax(unseen_predictions.logits, axis=1)\n",
    "\n",
    "sample_submission[\"target\"] = unseen_predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eea94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef4616",
   "metadata": {},
   "source": [
    "## Hugging Face with Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ccb0d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at hkayesh/twitter-disaster-nlp were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at hkayesh/twitter-disaster-nlp and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"hkayesh/twitter-disaster-nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa54db",
   "metadata": {},
   "source": [
    "### Hugging Face with Pre-trained Model - Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14387b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "results = []\n",
    "for text in df_test['text_with_lemmatization']:\n",
    "    result = pipe(text)[0]\n",
    "    results.append(result)\n",
    "\n",
    "sample_submission['target'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b40aba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_submission['target'] = sample_submission['target'].apply(extract_label)\n",
    "sample_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001979f",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f805f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466612af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(df_train['text_with_lemmatization'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "524e8e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 600)               0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " positional_embedding (Posi  (None, 600, 256)          5273600   \n",
      " tionalEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, 600, 256)          543776    \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 256)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5817633 (22.19 MB)\n",
      "Trainable params: 5817633 (22.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "167/167 [==============================] - ETA: 0s - loss: 0.7160 - accuracy: 0.6609INFO:tensorflow:Assets written to: model/full_transformer_encoder.x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/full_transformer_encoder.x/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 88s 526ms/step - loss: 0.7160 - accuracy: 0.6609 - val_loss: 0.5475 - val_accuracy: 0.7128\n",
      "Epoch 2/20\n",
      "167/167 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.8473INFO:tensorflow:Assets written to: model/full_transformer_encoder.x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/full_transformer_encoder.x/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 86s 515ms/step - loss: 0.3820 - accuracy: 0.8473 - val_loss: 0.5192 - val_accuracy: 0.7570\n",
      "Epoch 3/20\n",
      "167/167 [==============================] - 85s 509ms/step - loss: 0.2709 - accuracy: 0.8970 - val_loss: 0.5420 - val_accuracy: 0.7443\n",
      "Epoch 4/20\n",
      "167/167 [==============================] - 83s 499ms/step - loss: 0.1966 - accuracy: 0.9311 - val_loss: 0.6876 - val_accuracy: 0.7439\n",
      "Epoch 5/20\n",
      "167/167 [==============================] - 84s 501ms/step - loss: 0.1433 - accuracy: 0.9563 - val_loss: 0.7779 - val_accuracy: 0.7194\n",
      "Epoch 6/20\n",
      "167/167 [==============================] - 84s 501ms/step - loss: 0.0979 - accuracy: 0.9713 - val_loss: 1.0296 - val_accuracy: 0.6830\n",
      "Epoch 7/20\n",
      "167/167 [==============================] - 84s 503ms/step - loss: 0.0873 - accuracy: 0.9767 - val_loss: 0.8954 - val_accuracy: 0.6852\n",
      "Epoch 8/20\n",
      "167/167 [==============================] - 84s 503ms/step - loss: 0.0716 - accuracy: 0.9797 - val_loss: 1.0099 - val_accuracy: 0.6581\n",
      "Epoch 9/20\n",
      "167/167 [==============================] - 84s 502ms/step - loss: 0.0695 - accuracy: 0.9814 - val_loss: 0.9802 - val_accuracy: 0.6427\n",
      "Epoch 10/20\n",
      "167/167 [==============================] - 83s 500ms/step - loss: 0.0658 - accuracy: 0.9824 - val_loss: 1.0034 - val_accuracy: 0.6826\n",
      "Epoch 11/20\n",
      "167/167 [==============================] - 84s 504ms/step - loss: 0.0612 - accuracy: 0.9833 - val_loss: 1.0085 - val_accuracy: 0.6673\n",
      "Epoch 12/20\n",
      "167/167 [==============================] - 83s 500ms/step - loss: 0.0632 - accuracy: 0.9814 - val_loss: 1.0824 - val_accuracy: 0.6743\n",
      "Epoch 13/20\n",
      "167/167 [==============================] - 84s 503ms/step - loss: 0.0558 - accuracy: 0.9850 - val_loss: 1.0722 - val_accuracy: 0.6734\n",
      "Epoch 14/20\n",
      "167/167 [==============================] - 84s 501ms/step - loss: 0.0530 - accuracy: 0.9852 - val_loss: 1.0387 - val_accuracy: 0.5617\n",
      "Epoch 15/20\n",
      "167/167 [==============================] - 84s 504ms/step - loss: 0.0479 - accuracy: 0.9848 - val_loss: 0.9660 - val_accuracy: 0.6848\n",
      "Epoch 16/20\n",
      "167/167 [==============================] - 84s 504ms/step - loss: 0.0488 - accuracy: 0.9861 - val_loss: 1.1074 - val_accuracy: 0.6502\n",
      "Epoch 17/20\n",
      "167/167 [==============================] - 84s 504ms/step - loss: 0.0410 - accuracy: 0.9869 - val_loss: 1.1238 - val_accuracy: 0.6313\n",
      "Epoch 18/20\n",
      "167/167 [==============================] - 85s 509ms/step - loss: 0.0500 - accuracy: 0.9856 - val_loss: 0.9139 - val_accuracy: 0.6848\n",
      "Epoch 19/20\n",
      "167/167 [==============================] - 84s 506ms/step - loss: 0.0353 - accuracy: 0.9878 - val_loss: 1.0760 - val_accuracy: 0.6721\n",
      "Epoch 20/20\n",
      "167/167 [==============================] - 84s 504ms/step - loss: 0.0353 - accuracy: 0.9878 - val_loss: 0.9737 - val_accuracy: 0.6309\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No file or directory found at full_transformer_encoder.x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/full_transformer_encoder.x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                                     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mdf_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_with_lemmatization\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39mdf_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m---> 25\u001b[0m model2 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_transformer_encoder.x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     custom_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformerEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m: TransformerEncoder,\n\u001b[1;32m     28\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositionalEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: PositionalEmbedding})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    231\u001b[0m         filepath,\n\u001b[1;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    239\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at full_transformer_encoder.x"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorization(inputs)\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(x)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "model.fit(x=df_train['text_with_lemmatization'], y=df_train['target'], validation_split=0.3, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57792b49",
   "metadata": {},
   "source": [
    "### Transformer - Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c42ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 18s 178ms/step\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "threshold = 0.5\n",
    "\n",
    "sample_submission[\"target_float\"] = model.predict(df_test['text_with_lemmatization'])\n",
    "sample_submission[\"target\"] = sample_submission.apply(lambda df: 1 if df[\"target_float\"] > threshold else 0, axis=1)\n",
    "sample_submission = sample_submission.drop([\"target_float\"], axis=1)\n",
    "sample_submission.to_csv(\"data/submission.csv\", index=False)\n",
    "#sample_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
